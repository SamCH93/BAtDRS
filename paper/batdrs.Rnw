\documentclass[a4paper, 11pt]{article}

%% packages
%% ----------------------------------------------------------------------------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[title]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names

% fonts
% \usepackage{lmodern} % math and serif font
% \usepackage{mathpazo} % math and serif font
\usepackage{times} % math and serif font
\usepackage[semibold]{sourcesanspro} % sans serif font
\usepackage{sectsty} % use different fonts for different sections
\allsectionsfont{\sffamily} % for sections use sans serif
\usepackage[onehalfspacing]{setspace} % more space
\usepackage[labelfont=bf,font=small]{caption} % smaller captions

% tikz options
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,fit}
% define color for tikz figure
\definecolor{lightlightgray}{RGB}{211,211,211}
 % set default tikz font to sans serif
\tikzset{every picture/.style={/utils/exec={\sffamily}}}

%% other header configurations
%% ----------------------------------------------------------------------------
\input{header.tex}

\begin{document}
\maketitle

%% Disclaimer that a preprint
%% ----------------------------------------------------------------------------
\begin{center}
  \vspace{-2em}
  {\color{red}This is a preprint which has not yet been peer reviewed.}
\end{center}

\begin{center}
  \begin{minipage}{13cm}
    {\small
      \rule{\textwidth}{0.4pt} \\
      {\centering\textbf{Abstract}\\ Replication studies are essential to assess
        the credibility of claims from original studies. A critical aspect of
        designing a replication study is determining its sample
        size. % Too small
        % sample sizes may lead to inconclusive replications, whereas too large
        % sample sizes may lead to suboptimal allocation of resources.
        Here we show how Bayesian approaches can be used for this purpose. The
        Bayesian framework allows both the original study data and external
        knowledge to be incorporated into a design prior for the underlying
        parameters. This is particularly useful because external knowledge, such
        as expected heterogeneity between original and replication study due to
        differences in study execution and study population, are common in the
        replication setting. We investigate design priors in the normal normal
        hierarchical model where analytical results are available. Based on a
        design prior, predictions about the replication data can be made, and
        the replication sample size can be chosen to ensure a sufficiently high
        probability of replication success. Replication success may be defined
        through Bayesian or non-Bayesian criteria, and different criteria may
        also be combined to meet distinct stakeholders and allow conclusive
        inferences based on multiple analysis approaches. An application to data
        from a multisite replication project illustrates how the approach helps
        to design informative and cost-effective replication studies. The
        methods are made available in an R package. }
      \rule{\textwidth}{0.4pt} \\
      \textit{Keywords}: Bayesian design, design prior, multisite replication,
      sample size determination}
  \end{minipage}
\end{center}

<< "main-setup", echo = FALSE >>=
##  knitr options
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE,
               eval = TRUE)
@

<< "libraries-options" >>=
## packages
options(scipen = 5)
library(BayesRep)
library(ReplicationSuccess)
library(biostatUZH)
library(dplyr)
library(ggplot2)
@

<< "load-clean-data" >>=
## data and some computations
protzko <- read.csv("../data/protzko2020.csv")
protzkolong <- read.csv("../data/protzko2020long.csv")

## compute sceptical BF, replication BF, sceptical p
protzko$zo <- with(protzko, do/sqrt(vo))
protzko$zr <- with(protzko, dr/sqrt(vr))
protzko$c <- with(protzko, vo/vr)

protzko$BFs <- with(protzko, BFs(zo = zo, zr = zr, c = c))
protzko$BFr <- with(protzko, BFr(zo = zo, zr = zr, c = c, g = 0))
protzko$ps <- with(protzko, pSceptical(zo = zo, zr = zr, c = c,
                                       alternative = "one.sided",
                                       type = "golden"))
@

\section{Introduction}

The replicability of research findings is a cornerstone for the credibility of
science. However, there is growing evidence that the replicability of many
scientific findings is lower than expected \citep{Ioannidis2005,Opensc2015,
  Camerer2018, Errington2021}. This ``replication crisis'' has led to
methodological reforms in various fields of science, one of which is an
increased conduct of replication studies \citep{Munafo2017}. Statistical
methodology plays a key role in the evaluation of replication studies, and
various methods have been proposed for quantifying how ``successful'' a
replication study was in replicating the original finding \citep[among
others]{Bayarri2002, Verhagen2014, Simonsohn2015, Anderson2016, Patil2016,
  Johnson2016, Etz2016, vanAert2017, Ly2018, Harms2019, Hedges2019, Mathur2020,
  Held2020, Pawel2020, Held2021, Pawel2022b}. Yet, as with ordinary studies,
statistical methodology is not only important for analyzing replication studies
but also for designing them, in particular for their \emph{sample size
  determination} (SSD). Optimal SSD is important since too small sample sizes
may lead to inconclusive studies, whereas too large sample sizes may waste
resources which could have been allocated better in other research projects.

SSD for replication studies comes with unique opportunities and challenges; the
data from the original study can be used to inform SSD, at the same time the
analysis of the replication data is often different from an analysis of a study
in isolation. For these reasons, a relatively small literature has emerged which
specifically deals with replication study SSD for selected analysis methods and
data models. For instance, SSD for standardized mean difference effect sizes
analyzed with Bayes factors \citep{Bayarri2002}, SSD for statistical
significance assessment of the replication \citep{Goodman1992, Senn2002,
  Micheloud2020, vanZwet2022}, SSD for reverse-Bayes assessment of the
replication \citep{Held2020, Pawel2022b}, or SSD for meta-analysis of
replication studies \citep{Hedges2021}. The aim of this paper is to unify these
methods under a general framework. The proposed framework (schematically
illustrated in Figure~\ref{fig:SSDschema}) applies to any kind of data model and
analysis method, and is based on principles from Bayesian design analysis
\citep{Spiegelhalter1986, Spiegelhalter1986b, Spiegelhalter1986c, Weiss1997,
  OHagan2001b, DeSantis2004, Spiegelhalter2004, Schoenbrodt2017, Grieve2022}.
The design of replication studies is a natural candidate for Bayesian knowledge
updating. Specifically, the Bayesian framework allows to combine uncertain
information from different sources --for instance, the data from the original
study and/or expert knowledge-- in a so-called \emph{design prior} for the
underlying model parameters \citep{OHagan2001b}. Based on the design prior,
predictions about the replication data can be made, and the sample size can be
chosen such that the probability of replication success becomes sufficiently
high. Importantly, Bayesian design analysis can also be used if the planned
analysis of the replication study is non-Bayesian, which is the more common
situation in practice.

\begin{figure}[!tb]
  \centering
  {\small \input{tikzcode.tex} }
  \caption{Schematic illustration of the process of Bayesian sample size
    determination for replication studies.}
    % ; the observed data from the original study $x_{o}$ are used for
    % specifying a design prior for the model parameters $\theta$ (possibly along
    % with external knowledge). The design prior and an assumed replication sample
    % size $n_{r}$ then define a predictive distribution for the replication data
    % $X_{r}$. Based on this predictive distribution and a specified analysis
    % method (possibly involving the original data), the probability of
    % replication success can be computed. The replication sample size $n_{r}$ is
    % then determined such that the probability of replication success becomes
    % sufficiently large (and such that possible side constraints are satisfied).}
\label{fig:SSDschema}
\end{figure}

This paper is structured as follows: We start with presenting a general
framework for Bayesian SSD of replication studies (Section~\ref{sec:GFW}). We
then investigate design priors and sample size determination in the normal
normal hierarchical model and for several Bayesian and non-Bayesian analysis
methods (Section~\ref{sec:SS}). As a running example, we use data from a
cross-laboratory replication project \citep{Protzko2020}. Finally, the paper
ends with concluding remarks and practical recommendations (Section
\ref{sec:discussion}).


\section{General framework} \label{sec:GFW}

% Figure~\ref{fig:SSDschema} schematically illustrates the process of Bayesian SSD
% for replication studies, which we will explain in more detail in the following.
Suppose an original study has been conducted and resulted in a data set $x_{o}$.
These data are assumed to come from a distribution characterized by an unknown
parameter $\theta$ and with density function $f(x_{o} \given \theta)$. To assess
the replicability of a claim from the original study, an independent and
identically designed (apart from the sample size) replication study is
conducted, and the goal of the design stage is to determine its sample size
$n_{r}$.

As the observed original data $x_{o}$, the yet unobserved replication data
$X_{r}$ are assumed to come from a distribution depending on the parameter
$\theta$. The parameter $\theta$ thus provides a link between the two studies,
and the knowledge obtained from the original study can be used to make
predictions about the replication. The central quantity for doing so is the
so-called \emph{design prior} of the parameter $\theta$, which is the posterior
distribution of $\theta$ based on the original data and an initial prior for
$\theta$
\begin{align}
  \label{eq:dp}
  f(\theta \given x_o, \text{external knowledge}) =
  \frac{f(x_o \given \theta) \, f(\theta \given \text{external knowledge})}{f(x_o \given
  \text{external knowledge})}.
\end{align}
The initial prior of $\theta$ may depend on external knowledge (\eg{} data from
other studies), we will discuss common types of external knowledge in the
replication setting in Section~\ref{sec:SS}. The design prior~\eqref{eq:dp}
hence represents the state of knowledge and uncertainty about the parameter
$\theta$ before the replication is conducted, and, along with an assumed
replication sample size $n_{r}$, it can be used to compute a predictive
distribution for the replication data
\begin{align}
  \label{eq:ftr}
  f(x_r \given n_r, x_o, \text{external knowledge})
  &= \int%_\Theta
  % \underbrace{f(x_r \given n_{r}, \theta)}_{\text{likelihood}}
    f(x_r \given n_{r}, \theta)
    % \, \underbrace{f(\theta \given x_o, \text{external knowledge})}_{\text{design prior}}
    \, f(\theta \given x_o, \text{external knowledge})
    \, \text{d}\theta.
\end{align}

After completion of the replication, the observed data $x_r$ will be analyzed in
some way to quantify how much the original result could be replicated. The
analysis may involve the original data (for example, a meta-analysis of the two
data sets) or it may only use the replication data. Typically, there is a
\emph{success region} $S$ which implies that if the replication data fall within
it ($x_r \in S$), the replication is successful. The \emph{probability of
  replication success} can thus be computed by integrating the predictive
density~\eqref{eq:ftr} over $S$. To ensure a sufficiently conclusive replication
design, the sample size $n_{r}$ is determined such that the probability of
replication success is at least as large as a desired amount, here and
henceforth denoted by $1 - \beta$ (in analogy to the desired power in classical
SSD). The required sample size $n_r^*$ is then given by
\begin{align}
  \label{eq:nr}
  n_r^* = \inf \left\{n_r : \Pr(X_{r} \in S \given
  n_r, x_{o}, \text{external knowledge}) \geq 1 - \beta \right\}.
\end{align}

Often, replication studies are analyzed using several methods which quantify
different aspects of replicability, and which have different success regions
(\eg{} one method for quantifying parameter compatibility and another for
quantifying evidence against a null hypothesis). In this case, the sample size
may be chosen such that the probability of replication success is as large as
desired for all planned analysis methods.

There may sometimes also be side constraints which the replication sample size
needs to satisfy. For instance, funders and regulators may require from a method
to be \emph{calibrated} \citep{Grieve2016}, that is, to have appropriate type I
error rate control. A possible side constraint may thus be that the sample size
$n_{r}^{*}$ has to be determined such that the probability of replication
success under a null hypothesis is not larger than some desired level.
Similarly, it may be reasonable to have \emph{precision} constraints, \eg{} to
require that a confidence interval for the replication effect estimate has to be
at least as tight as the confidence interval estimated in the original study.
Finally, in most cases there is an upper limit on the possible sample size due
to limited resources and/or availability of samples.





\section{Sample size determination in the normal normal hierarchical model}
\label{sec:SS}

To conduct SSD for replication studies it is pragmatic to adopt a meta-analytic
perspective and use only study level summary statistics instead of the raw study
data since the raw data from the original study are not always available to the
replicators. Typically, the underlying parameter $\theta$ is a univariate effect
size quantifying the effect of an independent variable on the outcome variable
(\eg{} a mean difference, a log odds ratio, or a log hazard ratio). The original
and replication study can then be summarized through an effect estimate $\that$,
possibly the maximum likelihood estimate, and a corresponding standard error
$\sigma$, \ie{} $x_{o} = \{\that_{o}, \sigma_{o}\}$ and
$x_{r} = \{\that_{r}, \sigma_{r}\}$. Effect estimates and standard errors are
routinely reported in research articles or can, under some assumptions, be
computed from $p$-values and confidence intervals. As in the conventional
meta-analytic framework, we further assume that for study $k \in \{o, r\}$ the
(suitably transformed) effect estimate $\hat{\theta}_k$ is approximately
normally distributed around a study specific effect size $\theta_k$ and with
(known) variance equal to its squared standard error $\sigma_k^2$, here and
henceforth denoted by
$\hat{\theta}_k \given \theta_k \sim \Nor(\theta_k, \sigma_k^2)$. The standard
error $\sigma_k$ is typically of the form $\sigma_k = \lambda/\sqrt{n_k}$ with
$\lambda^{2}$ some unit variance and $n_{k}$ the sample size. The ratio of the
original to the replication variance is thus the ratio of the replication to the
original sample size
\begin{align*}
  c = \sigma^2_o/\sigma^2_r = n_r/n_o,
\end{align*}
which is often the main focus of SSD as it quantifies how much the replication
sample $n_{r}$ size needs to be changed compared to the original sample size
$n_{o}$. Depending on the effect size type, this framework might require slight
modifications \citep[see \eg{}][Chapter 2.4]{Spiegelhalter2004}.

Assuming a normal sampling model for the effect
estimates~\eqref{eq:hat_theta_k}, as described previously, and specifying an
initial hierarchical normal prior for the study specific effect
sizes~\eqref{eq:theta_k} and the effect size~\eqref{eq:theta}, leads then to the
\emph{normal normal hierarchical model}
\begin{subequations}
\label{eq:hierarch-model}
\begin{align}
  \hat{\theta}_k \given \mspace{-1mu} \theta_k &\sim \Nor(\theta_k, \sigma_k^2)
  \label{eq:hat_theta_k} \\
  \theta_k \given \theta \,\,  &\sim \Nor(\theta, \tau^2) \label{eq:theta_k} \\
  \theta \,\, &\sim \Nor(\mutheta,
  \sigmatheta^2). \label{eq:theta}
\end{align}
\end{subequations}
By marginalizing over the study specific effects sizes, the
model~\eqref{eq:hierarch-model} can alternatively be expressed as
\begin{subequations}
\label{eq:hierarch-model2}
\begin{align}
  \hat{\theta}_k \given \mspace{-1mu} \theta &\sim \Nor(\theta, \sigma_k^2 + \tau^{2})
  \label{eq:hat_theta_k2} \\
  \theta  &\sim \Nor(\mutheta,
  \sigmatheta^2) \label{eq:theta2}
\end{align}
\end{subequations}
which is often more useful for derivations and computations. In the following we
will explain how the normal normal hierarchical model can be used
% is similar to the random effects model typically used in meta-analysis, and
% to incorporate the original data and external knowledge into
for SSD of the replication study.

\subsection{Design prior and predictive distribution}

The observed original data $x_{o} = \{\that_{o}, \sigma_{o}\}$ can be combined
with the initial prior by Bayes' theorem~\eqref{eq:dp} to obtain a posterior
distribution for the effect size $\theta$
\begin{align}
  \label{eq:dpnormal}
  \theta \given \hat{\theta}_o, \sigma^2_o
  \sim
  \Nor\left(
  \frac{\hat{\theta}_o}{1 + 1/g} + \frac{\mutheta}{1 + g},
  \frac{\sigma^2_o + \tau^2}{1 + 1/g} \right)
\end{align}
where $g = \sigmatheta^2/(\sigma^2_o + \tau^2)$ is the \emph{relative prior
  variance}. This posterior serves then as the design prior for predicting the
replication data. Specifically, assuming a replication standard error
$\sigma_{r}$ and integrating the marginal density of the replication effect
estimate~\eqref{eq:hat_theta_k2} with respect to the design
prior~\eqref{eq:dpnormal} leads then to the predictive distribution
\begin{align}
  \label{eq:fthetar}
  \hat{\theta}_r \given \hat{\theta}_o, \sigma^2_o, \sigma^2_r
  \sim
  \Nor\left(\muthatr =
  \frac{\hat{\theta}_o}{1 + 1/g} + \frac{\mutheta}{1 + g}, \sigmathatr^{2} =
  \, \sigma^2_r + \tau^2 + \frac{\sigma^2_o + \tau^2}{1 + 1/g}\right).
\end{align}
For some analysis methods, replication success is more naturally defined via the
replication $z$-value $z_{r} = \that_{r}/\sigma_{r}$. In these cases, it can be
more convenient to use the associated predictive distribution of $z_{r}$
\begin{align}
  \label{eq:zr}
  z_r \given z_o, c%,  z_{\scriptscriptstyle{\theta}},h, g
  \sim
  \Nor\left\{\muzr =
  \sqrt{c} \left(\frac{z_o}{1 + 1/g} +
  \frac{z_{\scriptscriptstyle{\theta}} \sqrt{g  (1 + h)}}{1 + g}\right),
  \sigmazr^{2} =
  \, 1 + c \left( h + \frac{1 + h}{1 + 1/g}  \right) \right\}
\end{align}
which only depends on relative parameters, \ie{} the variance ratio
$c = \sigma^{2}_{o}/\sigma^{2}_{r}$, the original $z$-value
$z_{o} = \that_{o}/\sigma_{o}$, the prior $z$-value
$z_{\scriptscriptstyle{\theta}} = \mutheta/\sigma_{\scriptscriptstyle{\theta}}$,
and the relative heterogeneity $h = \tau^{2}/\sigma^{2}_{o}$. Both the design
prior~\eqref{eq:dp} and the predictive distributions~\eqref{eq:fthetar}
and~\eqref{eq:zr} depend on the parameters of the initial prior ($\tau^{2}$,
$\mutheta$, $\sigmatheta^{2}$). In the following, we will explain how these
parameters can be specified based on external knowledge.

\subsection{Incorporating external knowledge in the initial prior}
\label{sec:initialPrior}

At least three common types of external knowledge can be distinguished in the
replication setting: (i) expected heterogeneity between original and replication
study due to differences in study design, execution, and population, (ii) prior
knowledge about the effect size either from theory or from related studies,
(iii) scepticism regarding the original study due to the possibility of
exaggerated results. % The between-study heterogeneity variance $\tau^{2}$, the
% effect size mean $\mutheta$, and the effect size variance
% $\sigmatheta^2$ provide a means for incorporating these
% information in SSD of the replication study.

<< "prior-eliciation-values", fig.height = 3.5 >>=
## absolute approach
drange <- 0.2
tauAbs <- drange/(2*qnorm(p = 0.975))
tau <- tauAbs
@

\subsubsection{Between-study heterogeneity}
\label{sec:heterogeneity}
The expected degree of between-study heterogeneity can be incorporated via the
heterogeneity variance $\tau^2$ in~\eqref{eq:theta_k}. With smaller
heterogeneity variance $\tau^{2}$, the study specific effect sizes become more
similar, whereas for increasing $\tau^{2}$ they become more unrelated. If the
replicators do not expect any heterogeneity they can thus set $\tau^{2} = 0$
which will lead to the model collapsing to a fixed effects model.

If heterogeneity is expected, there are different approaches for specifying
$\tau^{2}$; A domain expert may subjectively assess how much heterogeneity is to
be expected due to the change in laboratory, study population, and other
factors. An alternative is to take an estimate from the literature, \eg{} from
multisite replication projects or from systematic reviews. Finally, one can also
specify an upper limit of ``tolerable heterogeneity''. This approach is similar
to specifying a minimal clinically relevant difference in classical power
analysis in the sense that a true replication effect size which is intolerably
heterogeneous from the original effect size is not relevant to be detected. An
absolute \citep[Chapter 5.7.3]{Spiegelhalter2004} and a relative approach
\citep{Held2020c} can be considered. In the absolute approach, a value of
$\tau^{2}$ is chosen such that a suitable range (\eg{} the IQR or the range from
2.5\% to 97.5\% of the distribution~\eqref{eq:theta_k}) of study-specific effect
sizes is not larger than an effect size difference considered negligible. For
example, when 95\% of the effect sizes should not vary more than a small effect
size \eg{} $d = \Sexpr{round(drange, 2)}$ on standardized mean difference scale
based on the \citet{Cohen1992} effect size classification, this would lead to
$\tau = d/(2 \cdot 1.96) \approx \Sexpr{round(tauAbs, 2)}$. In the relative
approach, $\tau^{2}$ is specified relative to the variance of the original
estimate $\sigma^{2}_{o}$ using field conventions for tolerable relative
heterogeneity. For example, in the Cochrane guidelines for systematic reviews
\citep{Deeks2019} a value of
$I^{2} = \tau^{2}/(\tau^{2} + \sigma^{2}_{o}) = 40\%$ is classified as
``negligible'', which translates to
$\tau^{2} = \sigma^{2}_{o}/(1/I^{2} - 1) = (2\sigma^{2}_{o})/3$.

We note that in principle it is also possible to assign a prior distribution to
$\tau^2$ \citep[see the literature from meta-analysis on this issue
\eg{}][]{Rover2021}. However, for interpretability reasons we will not consider
such an approach here as there are no closed-form expressions anymore for the
predictive distribution and the probability of replication success.

\subsubsection{Knowledge about the effect size}
Prior knowledge about the effect size $\theta$ can be incorporated via the prior
mean $\mutheta$ and prior variance $\sigmatheta^2$ in~\eqref{eq:theta}. For
instance, the parameters may be specified based on a meta-analysis of related
studies or based on expert elicitation. The resulting design prior will then
contain more information than what was provided by the original data alone,
leading to potentially more efficient designs. If there is no prior knowledge
available, one can specify an uninformative initial prior by letting the
variance go to infinity ($\sigmatheta^2 \to \infty$). The resulting design prior
will then only contain the information from the original study.

\subsubsection{Exaggerated original results}
\label{sec:shrinkage}
Potentially exaggerated original results can be counteracted by setting
$\mutheta = 0$ to obtain a shrinkage prior which shrinks the design prior
towards less impressive effect sizes than the observed one. For instance,
Replicators could believe that the results from the original study are
exaggerated because there is no pre-registered study protocol available. Even
without such beliefs, weakly informative shrinkage priors may also be motivated
from a ``regularization'' point of view as they will block physically impossible
parameter values from taking over the posterior in settings with uninformative
data \citep{Gelman2009}.

The amount of shrinkage is determined via the prior variance $\sigmatheta^2$. A
a diffuse prior ($\sigmatheta^2 \to \infty$) will lead to no shrinkage, while a
highly concentrated prior ($\sigmatheta^2 \downarrow 0$) will completely shrink
the design prior to a point mass. In practice, a pragmatic option with good
predictive properties is to use the empirical Bayes estimate based on the
original data
\begin{align}
  \label{eq:EBestimate}
  \hat{\sigma}^2_{\scriptscriptstyle{\theta}} = \max\{(\that_{o} - \mutheta)^{2} - \tau^{2} - \sigma^{2}_{o}, 0\}.
\end{align}
This choice will lead to adaptive shrinkage \citep{Pawel2020} in the sense that
shrinkage is large for unconvincing original studies (those with small effect
estimates $\that_{o}$ and/or large standard errors $\sigma_{o}$), but disappears
as the data become more convincing (through larger effect estimates $\that_{o}$
and/or smaller standard errors $\sigma_{o}$). Another option is to use an
estimate from a corpus of related studies \citep[\eg{} the Cochrane library of
systematic reviews as in][]{vanZwet2021}.



\subsection{Example: Cross-laboratory replication project}
\label{sec:example}
We will now illustrate the construction of design priors based on data from a
recently conducted replication project \citep{Protzko2020}, see
Figure~\ref{fig:data} for a summary of the data. The data were collected in four
laboratories. Each of them conducted four original studies and for each original
study four replication studies were carried out, one by the same lab and three
by the other three labs.

\begin{figure}[!tb]
<< "analyse-protzko", fig.height = 9.65 >>=

## compute heterogeneity estimates for external replications
MAerDF <- lapply(X = unique(protzko$Study), FUN = function(study) {
    dat <- subset(protzko, Study == study & Type == "Replication")
    n <- nrow(dat) ## number of external replications
    w <- 1/dat$vr ## fixed effects weights
    dFE <- sum(dat$dr*w)/sum(w) ## fixed effects estimate
    dFEse <- sqrt(1/sum(w)) ## standard error
    Q <- sum(w*(dat$dr - dFE)^2) ## Q-statistic
    tau2DL <- pmax(0, (Q - (n - 1))/(sum(w) - (sum(w^2)/sum(w))))
    tauDL <- sqrt(tau2DL) ## DL estimate
    data.frame(Study = study, dFE = dFE, dFEse = dFEse, tauDL = tauDL)
}) %>%
    bind_rows()

## compute fixed effects MA
protzko$dma <- with(protzko, (dr/vr + do/vo)/(1/vr + 1/vo))
protzko$sma <- with(protzko, sqrt(1/(1/vr + 1/vo)))
protzko$pma <- with(protzko, pnorm(q = dma/sma,
                                   lower.tail = ifelse(do > 0, FALSE, TRUE)))
protzko$pmaF <- with(protzko, paste(ifelse(pma < 0.0001, "italic(p['m'])",
                                          "italic(p['m']) =="),
                                   formatPval(x = pma)))

## compute BFee
## tauMT <- 0.05
## protzko$BFee <- with(protzko, BFee(to = do, tr = dr, so = sqrt(protzko$vo),
##                                    sr = sqrt(protzko$vr), tau = tauMT))

## format to display in plot
pr <- with(protzko, pnorm(q = dr/sqrt(vr),
                          lower.tail = ifelse(do > 0, FALSE, TRUE)))
protzko$prF <- with(protzko, paste(ifelse(pr < 0.0001, "italic(p['r'])",
                                          "italic(p['r']) =="),
                                   formatPval(x = pr)))
## protzko$BFeeF <- with(protzko,
##                       ifelse(is.na(BFee), "BF['E'] ~ 'undefined'",
##                              paste(ifelse(BFee < 1/1000, "BF['E']", "BF['E'] =="),
##                                    formatBF(BF = BFee))))
protzko$BFsF <- with(protzko,
                     ifelse(is.na(BFs), "BF['S'] ~ 'undefined'",
                                  paste(ifelse(BFs < 1/1000, "BF['S']", "BF['S'] =="),
                                        formatBF(BF = BFs))))
protzko$BFrF <- with(protzko, paste(ifelse(BFr < 1/1000, "BF['R']", "BF['R'] =="),
                                    formatBF(BF = BFr)))
protzko$psF <- with(protzko, paste(ifelse(ps < 0.0001, "italic(p)['S']",
                                          "italic(p)['S'] =="),
                                   formatPval(x = ps)))
protzko$cF <- with(protzko, paste("italic(c) ==", round(c, 1)))

protzkolong$Studyord <- factor(protzkolong$Study,
                               levels = unique(protzko$Study)[
                                   order(abs(unique(protzko$zo)))])
protzko$Studyord <- factor(protzko$Study,
                           levels = unique(protzko$Study)[
                               order(abs(unique(protzko$zo)))])

## plot effect estimates stratified by experiment and lab
library(ggplot2)
protzkolong %>%
    mutate(Type = case_when(Type == "Original" ~ "original study",
                            Type == "Replication" ~ "external-replication",
                            Type == "Self-Replication" ~ "self-replication"),
           Type = factor(Type, levels = c("original study", "external-replication",
                                          "self-replication"))) %>%
  ggplot(aes(x = dr, y = Rlab)) +
    geom_hline(yintercept = c(1.4, 2.4, 3.4), alpha = 0.1, size = 0.3) +
    geom_vline(xintercept = 0, lty = 2, alpha = 0.3, size = 0.5) +
    ## geom_text(data = protzko, aes(x = 0.63+0.12, label = BFsF), parse = TRUE,
    ##           size = 2.2, nudge_y = 0.22, hjust = 0) +
    geom_text(data = protzko, aes(x = 0.63+0.12, label = BFrF), parse = TRUE,
              size = 2.2, nudge_y = -0.44, hjust = 0) +
    ## geom_text(data = protzko, aes(x = 0.63+0.12, label = BFeeF), parse = TRUE,
    ##           size = 2.2, nudge_y = -0.22, hjust = 0) +
    geom_text(data = protzko, aes(x = 0.63+0.16, label = pmaF), parse = TRUE,
              size = 2.2, nudge_y = -0.22, hjust = 0) +
    geom_text(data = protzko, aes(x = 0.63+0.18, label = prF), parse = TRUE,
              size = 2.2, nudge_y = 0, hjust = 0) +
    geom_text(data = protzko, aes(x = 0.63+0.19, label = cF), parse = TRUE,
              size = 2.2, nudge_y = 0.22, hjust = 0) +
    geom_pointrange(aes(xmin = dr - 1.96*sqrt(vr), xmax = dr + 1.96*sqrt(vr),
                        col = Type),
                    position = position_dodge(width = 0.5), fatten = 2,
                    key_glyph = "point") +
    scale_x_continuous(breaks = seq(0, 0.6, 0.3), minor_breaks = NULL) +
    coord_cartesian(xlim = c(-0.15, 1.2)) +
    scale_color_brewer(palette = "Dark2") +
    facet_wrap(~ Studyord) +
    labs(y = "Laboratory", x = "SMD effect estimate with 95% CI", color = "") +
    theme_bw() +
    theme(panel.grid.major.y = element_blank(),
          panel.grid.minor.y = element_blank(),
          legend.position = "top")
@
\caption{Data from cross-laboratory replication project by \citet{Protzko2020}.
  Shown are standardized mean difference (SMD) effect estimates with 95\%
  confidence intervals stratified by experiment and laboratory. For each
  replication study, the relative sample size $c = n_{r}/n_{o}$, the one-sided
  replication $p$-value $p_{r}$, the one-sided meta-analytic $p$-value $p_{m}$,
  and the replication Bayes factor $\BFr$ are shown. Experiments are ordered by
  the magnitude of their original $z$-value
  $z_{o} = \hat{\theta}_{o}/\sigma_{o}$.}
\label{fig:data}
\end{figure}


Most studies used simple between-subject designs with two groups and a
continuous outcome so that for a study $i \in \{o, r\}$ the standardized mean
difference (SMD) effect estimate $\that_{i}$ can be computed from the group
means $\bar{y}_{i1}, \bar{y}_{i2}$, group standard deviations $s_{i1}, s_{i2}$,
and group sample sizes $n_{i1}, n_{i2}$ by
\begin{align*}
  \that_{i} = \frac{\bar{y}_{i1} - \bar{y}_{i2}}{s_{i}}
\end{align*}
with
$s^{2}_{i} = \{(n_{i1} - 1)s_{i1}^{2} + (n_{i2} - 1)s_{i2}^{2}\}/(n_{i1} + n_{i2} - 2)$
the pooled sample variance. Under a normal likelihood and assuming equal
variances in both groups, the approximate variance of $\that_{i}$ is
\begin{align}
  \sigma^{2}_{i} = \frac{n_{i1} + n_{i2}}{n_{i1}n_{i2}} + \frac{\that_{i}^{2}}{2(n_{i1} + n_{i2})}
  \label{eq:varSMD}
\end{align}
\citep{Hedges1981}. A cruder, but for SSD more useful, approximation
$\sigma^{2}_{i} \approx 4/n_{i}$ is obtained by assuming the same sample size in
both groups $n_{i1} = n_{i2} = n_{i}/2$, with $n_{i}$ the total sample size, and
neglecting the second term in~\eqref{eq:varSMD} which will be close to zero for
small effect estimates and/or large sample sizes \citep{Hedges2021}. We thus
have the approximate unit variance $\lambda^{2} = 4$ and the relative variance
\begin{align*}
  c = \sigma^{2}_{o}/\sigma^{2}_{r} = n_{r}/n_{o},
\end{align*}
which can be interpreted as the ratio of the replication to the original sample
size.
\begin{figure}[!htb]
<< "example", fig.height = 3.5 >>=
## example study for illustration of design prior
studyname <- "Labels"
study <- subset(protzkolong, Study == studyname & Type == "Original")

## uninformative initial prior
meanDP <- study$dr
varDP <- study$vr
varDPhet <- study$vr + tauAbs^2

## empirical Bayes shrinkage prior
shrinkEB <- pmax(0, 1 - study$vr/study$dr^2)
meanEB <- shrinkEB*study$dr
varDPEB <- shrinkEB*study$vr
shrinkEBhet <- pmax(0, 1 - (study$vr + tauAbs^2)/study$dr^2)
meanEBhet <- shrinkEBhet*study$dr
varDPEBhet <- shrinkEBhet*(study$vr + tauAbs^2)

## optimistic initial prior
extstudy <- subset(protzkolong, Study == studyname & Type == "Self-Replication")
varDPo <- 1/(1/study$vr + 1/extstudy$vr)
meanDPo <- varDPo*(study$dr/study$vr + extstudy$dr/extstudy$vr)
varDPohet <- 1/(1/(study$vr + tauAbs^2) + 1/extstudy$vr)
meanDPohet <- varDPohet*(study$dr/(study$vr + tauAbs^2) + extstudy$dr/extstudy$vr)
cSelf <- study$vr/extstudy$vr

## plot the priors
thetaseq <- seq(-0.05, 0.45, length.out = 500)
applyGrid <- data.frame(mean = c(meanDP, meanDP,
                                 meanEB, meanEBhet,
                                 meanDPo, meanDPohet),
                        var = c(varDP, varDPhet,
                                varDPEB, varDPEBhet,
                                varDPo, varDPohet),
                        tau = c(0, tauAbs,
                                0, tauAbs,
                                0, tauAbs),
                        initial = c("uninformative", "uninformative",
                                    "shrinkage", "shrinkage",
                                    "optimistic", "optimistic"))
plotDF <- do.call("rbind",
                    lapply(X = seq(1, nrow(applyGrid)), FUN = function(i) {
    dens <- dnorm(x = thetaseq, mean = applyGrid$mean[i],
                  sd = sqrt(applyGrid$var[i]))
    data.frame(theta = thetaseq, density = dens,
               applyGrid[i,])
}))
npriors <- length(unique(plotDF$initial))
colsPriors <- palette.colors(n = npriors + 1, palette = "Okabe-ito")[2:(npriors + 1)]
names(colsPriors) <- unique(plotDF$initial)

ggplot(data = plotDF, aes(x = theta, y = density, color = initial)) +
    geom_line(alpha = 0.9, size = 0.8) +
    facet_wrap(~ tau, labeller = label_bquote(tau == .(round(tau, 2)))) +
    labs(x = bquote("Effect size" ~ theta ~ "(SMD)"),
         y = "Density", color = "") +
    scale_color_manual(values = colsPriors) +
    theme_bw() +
        theme(legend.position = "top", panel.grid.minor = element_blank())
@

\caption{Design priors for the effect size $\theta$ (SMD) in the experiment
  ``\Sexpr{studyname}'' based on the original effect estimate
  $\hat{\theta}_{o} = \Sexpr{round(study[,"dr"], 2)}$ with standard error
  $\sigma_{o} = \Sexpr{round(sqrt(study[,"vr"]), 2)}$. Shown are different
  choices of the initial prior for $\theta$ and the between-study heterogeneity $\tau$.}
\label{fig:dpexample}
\end{figure}



Suppose now the original studies have been finished, and we want to conduct SSD
for the not yet conducted replication studies. We start by specifying the design
priors (one for each replication). Since the original studies have been
preregistered, we do not expect an exaggeration of their effect estimates due to
selective reporting or other questionable research practices. Therefore, we
choose an uninformative initial prior ($g \to \infty$), which leads to design
prior and predictive distribution both centered around the original effect
estimate $\that_{o}$.

Concerning the specification of between-study heterogeneity, a distinction needs
to be made between replications which are conducted in the same lab as the
original study (\emph{self-replications}) and replications which are conducted
in a different lab (\emph{external-replications}). For self-replications it is
reasonable to set $\tau^{2} = 0$ because we would expect no between-study
heterogeneity as the experimental conditions will be nearly identical in both
studies. In contrast, one would expect some between-study heterogeneity for
external-replications as the experimental conditions may slightly differ between
the labs. In the following, we will use $\tau^{2} = \Sexpr{round(tauAbs, 2)}$
elicited via the ``absolute'' approach as discussed in
Section~\ref{sec:heterogeneity}, since it is independent of the sample size of
the original study.


Taken together, we obtain the design prior
$\theta \given \hat{\theta}_{o}, \sigma^{2}_{o} \sim \Nor(\hat{\theta}_{o}, \sigma^{2}_{o})$
for self-replications and the design prior
$\theta \given \hat{\theta}_{o}, \sigma^{2}_{o} \sim \Nor(\hat{\theta}_{o}, \sigma^{2}_{o} + \tau^{2})$
for external-replications. For example, for the experiment named
``\Sexpr{studyname}'', the design prior would be centered around the original
effect estimate $\hat{\theta}_{o} = \Sexpr{round(study[,"dr"], 3)}$ with
variance
$\sigma^{2}_{o} + \tau^{2} = \Sexpr{round(sqrt(study[,"vr"]), 2)}^{2} + \Sexpr{round(tauAbs, 2)}^{2} = \Sexpr{round(sqrt(varDPhet), 2)}^{2}$
for an external-replication, and with variance
$\sigma^{2}_{o} = \Sexpr{round(sqrt(study[,"vr"]), 2)}^{2}$ for a
self-replication. Figure~\ref{fig:dpexample} (yellow lines) shows the two priors.
% Note that power
% calculations under the former prior have been termed ``predictive power''
% \citep{Micheloud2020}, while the latter prior has not appeared in the SSD for
% replication studies literature.

If there would have been good reason to believe that the original result may have
been exaggerated, we might have specified an initial shrinkage
prior. For instance, using the empirical Bayes estimate~\eqref{eq:EBestimate}
for the prior variance leads to a prior with shrinkage factor
$\hat{g}/(1 + \hat{g}) = \Sexpr{round(shrinkEBhet, 2)}$ for
an external-replication. The mean and variance are then shrunken towards
zero by $\Sexpr{round((1 - shrinkEBhet)*100, 0)}$\% compared to the mean and
variance of the design prior based on the uninformative initial prior (the blue lines in  Figure~\ref{fig:dpexample}). Conversely, if we had prior knowledge about the
effect size $\theta$ from another study, we could have specified an initial optimistic
prior. Suppose, for instance, that the self-replication of the experiment
``\Sexpr{studyname}'' was a pilot study, and its effect estimate
$\hat{\theta}_{p} = \Sexpr{round(extstudy[,"dr"], 3)}$ and standard error
$\sigma_{p} = \Sexpr{round(sqrt(extstudy[,"vr"]), 2)}$ were available to us.
This would lead to a design prior centered around the weighted mean of
original and pilot study, as well as, a prior precision equal to the sum of the
precision of both estimates (green lines in Figure~\ref{fig:dpexample}). Due to
incorporation of the external data, this design prior is much more concentrated
than the other two.


\subsection{Probability of replication success and required sample size}
To compute the probability of replication success one needs to select an
analysis methods and integrate the predictive distribution~\eqref{eq:fthetar}
% , respectively, \eqref{eq:zr}
 over the associated success region $S$. There is no universally accepted method
for quantifying replicability and we do not want to contribute to the discussion
which method is the most appropriate. In the following, we simply show the
success regions of different methods, and how the replication sample size can be
computed from them.


\subsubsection{The two-trials rule}
The most common approach for analysis of replication studies is to declare
replication success when both the original and replication study lead to a
$p$-value for testing the null hypothesis $H_{0} \colon \theta = 0$ smaller than a
pre-specified threshold $\alpha$, usually $\alpha = 5\%$ for two-sided tests and
$\alpha = 2.5\%$ for one-sided tests. This procedure is known as the
\emph{two-trials rule} in drug regulation \citep{Senn2008}.

We now assume the original study led to a positive effect estimate $\that_{o} > 0$ and
the corresponding one-sided $p$-value was significant at some level $\alpha$, \ie{}
$p_{o} = 1 - \Phi(\that_{o}/\sigma_{o}) \leq \alpha$. Replication success at level
$\alpha$ with the two-trials is then achieved if the replication $p$-value is also
significant, \ie{} $p_{r} = 1 - \Phi(\that_{r}/\sigma_{r}) \leq \alpha$, which
implies a success region
\begin{align}
  \label{eq:S2TR}
  S_{\scriptscriptstyle \text{2TR}}
  = \left[\zalpha\, \sigma_{r}, \infty \right),
\end{align}
where $\zalpha$ is the $1 - \alpha$ quantile of the standard
normal distribution. The probability of replication success is thus given by
\begin{align}
  \label{eq:2TRpros}
  \Pr(\that \in S_{\scriptscriptstyle \text{2TR}} \given \that_{o}, \sigma_{o}, \sigma_{r})
  = \Phi\left(\frac{\muthatr - \zalpha \, \sigma_{r}}{\sqrt{\sigma^{2}_{r} +
\tau^{2} + (\sigma^{2}_{o} + \tau^{2})/(1 + 1/g)}}\right)
\end{align}
with $\Phi(\cdot)$ the standard normal cumulative distribution function and
$\muthatr$ the mean of the predictive distribution~\eqref{eq:fthetar}.
Importantly, by decreasing the standard error $\sigma_{r}$ (through increasing
the sample size $n_{r}$), the probability of replication
success~\eqref{eq:2TRpros} cannot become arbitrarily large but is bounded by
\begin{align}
  \label{eq:limP2TR}
  \text{limP}_{\scriptscriptstyle \text{2TR}} =
  \Phi\left(\frac{ \muthatr}{\sqrt{
\tau^{2} + (\sigma^{2}_{o} + \tau^{2})/(1 + 1/g)}}\right).
\end{align}
The required replication standard error $\sigma_{r}^{*}$ to achieve replication
success with probability
$1 - \beta < \text{limP}_{\scriptscriptstyle \text{2TR}}$ can now be obtained by
equating~\eqref{eq:2TRpros} to $1 - \beta$ and solving for $\sigma_{r}$. This
leads to
\begin{align}
  \label{eq:ssd2tr}
  \sigma_{r}^{*} =
  \frac{\muthatr \zalpha + \zbeta\sqrt{
  (\zalpha^{2} - \zbeta^{2})\left\{\tau^{2} + (\sigma^{2}_{o} +
  \tau^{2})/(1 + 1/g)\right\} + \muthatr^{2}}}{\zalpha^{2} - \zbeta^{2}}
\end{align}
for $\beta < \alpha$. The standard error $\sigma_{r}^{*}$ can subsequently
be translated in a sample size, the translation depending on the type of effect
size (\eg{} for SMD effect sizes by $n_{r}^{*} \approx \lceil4/(\sigma^*_{r})^{2}\rceil$).

\subsubsection{Fixed effects meta-analysis}
Original and replication effect estimates have been analyzed via fixed-effects
meta-analysis. The pooled effect estimate $\that_{m}$ and standard error
$\sigma_{m}$ are given by
\begin{align*}
  &\that_{m} =
    \left(\that_{o}/\sigma_{o}^{2} + \that_{r}/\sigma^{2}_{r}\right)\sigma^{2}_{m}&
&\text{and}&                                                                                       &\sigma_{m} = \left(1/\sigma^{2}_{o} + 1/\sigma^{2}_{r}\right)^{-1/2},&
\end{align*}
and are equivalent to the mean and standard deviation of a posterior
distribution for the effect size $\theta$ based on the data from both studies
and an initial flat prior for $\theta$. Consequently, the
success region for the replication effect estimate $\that_{r}$
\begin{align}
  \label{eq:SMA}
  S_{\scriptscriptstyle \text{MA}}
  = \left[\sigma_{r} \zalpha\sqrt{1 + \sigma^{2}_{r}/\sigma^{2}_{o}} -
  (\that_{o} \sigma^{2}_{r})/\sigma^{2}_{o},
  \infty \right)
\end{align}
corresponds to both replication success defined via a one-sided meta-analytic
$p$-value being smaller than a level $\alpha$,
\ie{} $p_{m} = 1 - \Phi(\that_{m}/\sigma_{m}) \leq \alpha$, or to replication
success defined via a Bayesian posterior probability
$\Pr(\theta > 0 \given \that_{o}, \that_{r}, \sigma_{o}, \sigma_{r}) \geq 1 - \alpha$.
From the success region~\eqref{eq:SMA} and an assumed standard error $\sigma_{r}$
the probability of replication success can be computed by
\begin{align}
  \label{eq:MApros}
  \Pr(\that \in S_{\scriptscriptstyle \text{MA}} \given \that_{o}, \sigma_{o}, \sigma_{r})
  = \Phi\left(\frac{\muthatr - \sigma_{r} \zalpha\sqrt{1 + \sigma^{2}_{r}/\sigma^{2}_{o}}
  + (\that_{o} \sigma^{2}_{r})/\sigma^{2}_{o}}{\sqrt{\sigma^{2}_{r} +
\tau^{2} + (\sigma^{2}_{o} + \tau^{2})/(1 + 1/g)}}\right).
\end{align}
As for the two-trials rule, the probability~\eqref{eq:MApros} cannot be made larger
than~\eqref{eq:limP2TR} by decreasing the standard error $\sigma_{r}$.
and the required standard error $\sigma_{r}^{*}$ for it to be sufficiently large
can be computed numerically using a root finding algorithm.


\subsubsection{Effect size difference equivalence test}

\begin{align}
  \label{eq:equivalence}
  S_{\scriptscriptstyle \text{E}}
  = \left[\that_{o} - \Delta - \zalpha \sqrt{\sigma^{2}_{o} +
  \sigma^{2}_{r}}, \that_{o} + \Delta - \zalpha
  \sqrt{\sigma^{2}_{o} + \sigma^{2}_{r}}\right]
\end{align}

\subsubsection{The replication Bayes factor}

\begin{align}
  \label{eq:BFrsuccess}
  S_{\scriptscriptstyle \BFr}
  = \left(-\infty, -\sqrt{A} - \that_{r}/c\right] \bigcup
   \left[\sqrt{A} - \that_{r}/c, \infty\right)
\end{align}
with $A = \sigma^{2}_{r}\{z_{o}^{2} - 2 \log \gamma + \log(1 + c)\}(1 + 1/c)$

\subsubsection{The sceptical \textit{p}-value}

\begin{align}
  \label{eq:Pssuccess}
  S_{\scriptscriptstyle \ps}
  = \left[\sigma_{r} \zalpha \sqrt{1 +
  c/\{(z_{o}^{2}/\zalpha^{2}) - 1\}}, \infty\right)
\end{align}

\subsubsection{The sceptical Bayes factor}

\begin{align}
  \label{eq:BFssuccess}
  S_{\scriptscriptstyle \BFs}
  =
\end{align}

\section{Discussion}
\label{sec:discussion}

We have presented a Bayesian approach for SSD of replication studies. The
Bayesian framework allows to make use of all the available information, and to
take into account the associated uncertainty. We have also discussed how
different design requirements can be combined to satisfy different stakeholders,
while also enabling conclusive inferences based on several analyses approaches
of the replication data. As we showed, the approach helps to design informative
and cost-effective replications. We have illustrated the approach for three
Bayesian measures of replication success, but in principle our framework can be
used for any analysis method, Bayes or non-Bayes, parameter estimation or
hypothesis testing.

There are some limitations and possible extensions: We have treated all
variances as fixed in order to obtain closed form expressions for the
probability of replication success. Also specifying priors on the between-study
heterogeneity variances could better reflect the available uncertainty but would
come at the price of lower interpretability and higher computational complexity.
We have also not considered designs where the replication data are analyzed in a
sequential manner. Ideas from the Bayesian sequential design
\citep{Schoenbrodt2017} or from the adaptive trials literature \citep{Bretz2009}
could be adapted to the replication setting as in \citet{Micheloud2020}. A
sequential analysis of the replication data could possibly increase the
efficiency of the replication. However, it would also make SSD and practical
aspects more challenging. Moreover, we have assumed that the original study has
already been finished. One could also consider a scenario where both the
original and replication study are planned simultaneously and adopt a
``project'' perspective as in \citet{Held2021}. However, in this case no
information from the original study is available and the design prior needs to
be specified entirely based on external knowledge. Finally, researchers have
only limited resources and it may happen that they cannot afford a large enough
sample size to obtain their desired probability of replication success. In this
situation a reverse-Bayes approach \citep{Held2021b} could be applied in order
to determine the prior for the effect size which is required to meet all design
requirements based on a fixed sample size. Researchers can then judge whether or
not such prior beliefs are scientifically sensible, and decide whether they
should conduct the replication study with their limited resources.

\section*{Software and data}
The data from \citet{Protzko2020} were were downloaded from
\url{https://osf.io/42ef9/}. All analyses were conducted in the R programming
language version \Sexpr{paste(version[["major"]], version[["minor"]], sep =
  ".")} \citep{R}. The code to reproduce this manuscript is available at
\url{https://github.com/SamCH93/BAtDRS}. A snapshot of the Git repository at the
time of writing this article is archived at
\url{https://doi.org/10.5281/zenodo.XXXXXX}. Methods for Bayesian SSD of
replication studies are implemented in the R package \texttt{BayesRepDesign}
which is available at \url{https://github.com/SamCH93/BayesRepDesign}.
Appendix~\ref{app:package} illustrates the basic usage of the package.

\section*{Acknowledgments}
This work was supported by the Swiss National Science Foundation(\#189295). The
funder had no role in study design, data collection, data analysis, data
interpretation, decision to publish, or preparation of the manuscript. We thank
\citet{Protzko2020} for publicly sharing their data. We thank Charlotte
Micheloud for helpful comments on drafts of the manuscript.
% Our acknowledgement of these individuals does not imply their endorsement of this article.


% Appendix
% ------------------------------------------------------------------------------
\begin{appendices}

\section{Multisite Bayes factors}
\label{app:multiBFr}
To determine the multisite version of the replication and the sceptical Bayes
factor we need to know the marginal density of the replication effect estimates
$\hat{\theta}_r \given \theta \sim \Nor_{n}(\theta J_{n}, \text{diag}\left\{\sigma_r^2 + \tau^2_{r} J_{n}\right\})$
under a normal prior $H_{k} \colon \theta \sim \Nor(m, v)$. Let $\Nor(x;m,v)$
denote the normal density function mean $m$ and variance $v$ evaluated at $x$.
Define also
$\hat{\theta}_{r*} = \left\{\sum_{i=1}^{n}\hat{\theta}_{ri}/(\sigma^{2}_{ri} + \tau^{2}_{r})\right\} \sigma^{2}_{r*}$
and
$\sigma^{2}_{r*} = 1/\left\{\sum_{i=1}^{n}1/(\sigma^{2}_{ri} + \tau^{2}_{r})\right\}$,
\ie{} the weighted average of the replication effect estimates and its variance.
The marginal density is then given by
\begin{align*}
  f(\hat{\theta}_{r} \given H_{k})
  &= \int f(\hat{\theta}_{r} \given \theta) f(\theta \given H_{k})
    \, \text{d}\theta \\
  &= \int \frac{\exp\left[-\frac{1}{2} \left\{\sum_{i=1}^{n} \frac{(\hat{\theta}_{ri} - \theta)^{2}}{\sigma^{2}_{ri} + \tau^{2}_{r}} +
    \frac{(\theta - m)^{2}}{v}\right\} \right]}{
    \left\{2\pi v \prod_{i = 1}^{n} 2\pi \left(\sigma^{2}_{ri} + \tau^{2}_{r}\right)\right\}^{1/2}}
    \, \text{d}\theta \\
  &= \int \frac{
    \exp\left[-\frac{1}{2} \left\{\sum_{i=1}^{n} \frac{(\hat{\theta}_{ri} - \hat{\theta}_{r*})^{2}}{\sigma^{2}_{ri} + \tau^{2}_{r}} +  \frac{(\hat{\theta}_{r*} - \theta)^{2}}{\sigma^{2}_{r*}} +
    \frac{(\theta - m)^{2}}{v}\right\} \right]}{
    \left\{2\pi v \prod_{i = 1}^{n} 2\pi \left(\sigma^{2}_{ri} + \tau^{2}_{r}\right)\right\}^{1/2}}
    \, \text{d}\theta \\
  &= \frac{\exp\left[-\frac{1}{2} \left\{\sum_{i=1}^{n} \frac{(\hat{\theta}_{ri} - \hat{\theta}_{r*})^{2}}{\sigma^{2}_{ri} + \tau^{2}_{r}}\right\} \right]}{\left\{2\pi v \prod_{i = 1}^{n} 2\pi \left(\sigma^{2}_{ri} + \tau^{2}_{r}\right)\right\}^{1/2}}
    \underbrace{\int \exp\left[-\frac{1}{2} \left\{\frac{(\hat{\theta}_{r*} - \theta)^{2}}{\sigma^{2}_{r*}} +
    \frac{(\theta -m)^{2}}{v}\right\} \right] \text{d}\theta}_{= \Nor(\hat{\theta}_{r*}; m, v + \sigma^{2}_{r*}) 2\pi \sqrt{v} \sigma_{r*}} \\
  &= \left\{(1 + v/\sigma^{2}_{r*}) \prod_{i = 1}^{n} 2\pi \left(\sigma^{2}_{ri} + \tau^{2}_{r}\right)\right\}^{-1/2} \exp\left[-\frac{1}{2}\left\{
    \sum_{i=1}^{n} \frac{(\hat{\theta}_{ri} - \hat{\theta}_{r*})^{2}}{\sigma^{2}_{ri} + \tau^{2}_{r}} + \frac{(\hat{\theta}_{r*} - m)^{2}}{\sigma^{2}_{r*} + v}\right\}\right].
\end{align*}
The marginal density under the sceptical prior $\HS$ is then obtained by setting
$m = 0$ and $v = s \sigma^{2}_{o}$, whereas the marginal density under the
advocacy prior $\HA$ is obtained by setting $m = \that_{o}$ and
$v = \sigma^{2}_{o}$. The marginal density under the null hypothesis $H_{0}$ is
itself a special case of the density under $\HS$ with $s = 0$. Taken together,
this leads to the Bayes factor
\begin{align*}
      \BF_{\text{SA}}(\hat{\theta}_{r}; s)
  &= \frac{f(\hat{\theta}_{r} \given \HS)}{f(\hat{\theta}_{r} \given \HA)}
    = \sqrt{\frac{1 + \sigma^{2}_{o}/\sigma^{2}_{r*}}{1 +
    s \sigma^{2}_{o}/\sigma^{2}_{r*}}}  \exp\left[-\frac{1}{2}\left\{
    \frac{\hat{\theta}_{r*}^{2}}{\sigma^{2}_{r*} + s \sigma^{2}_{o}} -
    \frac{(\hat{\theta}_{r*} - \hat{\theta}_{o})^{2}}{\sigma^{2}_{r*} + \sigma^{2}_{o}}\right\}\right]
\end{align*}
which can be further simplified to~\eqref{eq:bfmulti}.

\section{The BayesRepDesign R package}
\label{app:package}


\end{appendices}


%% Bibliography
%% -----------------------------------------------------------------------------
\bibliographystyle{apalikedoiurl}
\bibliography{bibliography}

\end{document}
